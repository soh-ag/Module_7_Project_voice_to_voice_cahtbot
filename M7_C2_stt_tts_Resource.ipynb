{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Speech-to-Text & Text-to-Speech (Hugging Face) — Colab-ready Notebook\n",
        "\n",
        "This notebook is a teaching resource for a class on **Speech-to-Text (ASR)** and **Text-to-Speech (TTS)**. It uses **Hugging Face models** and other open-source tools, with examples for **English** and **Bengali (Bangla)**.\n",
        "\n",
        "Features included:\n",
        "- Install & setup (Colab-friendly)\n",
        "- ASR: Whisper (multilingual) + Bengali Wav2Vec2 models\n",
        "- TTS: Facebook MMS (English & Bengali) + community Bengali TTS\n",
        "- Voice cloning / zero-shot cloning using Coqui XTTS (local in Colab)\n",
        "- Saving and playing audio in the notebook\n",
        "\n",
        "Notes:\n",
        "- First run requires downloading models (internet). After that, caching enables offline use.\n",
        "- Stable GPU (Colab GPU) is recommended for faster model performance but not required for all examples.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7As-m2D8YxId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Install required libraries (run in Colab)"
      ],
      "metadata": {
        "id": "_eic6XfIZF3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these in Colab (uncomment to execute)\n",
        "# !pip install -q  transformers==4.39.3 datasets==2.18.0 soundfile==0.12.1 librosa==0.10.2\n",
        "!pip install -q \"huggingface_hub>=0.14.1\"\n",
        "# !pip install -q transformers[speech]\n",
        "!pip install -q git+https://github.com/coqui-ai/TTS.git@main  # for XTTS voice-cloning\n",
        "!pip install -q soundfile\n",
        "!pip install -q ipywidgets\n",
        "# !pip install -q torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "P-T9fQPxZWWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text -> speech (tts) | speech -> text (stt) | chatbot (text) -> text"
      ],
      "metadata": {
        "id": "Ws7WSRcC06FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "openai realtime api -> speech - speech"
      ],
      "metadata": {
        "id": "_6qe7exQ1LyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you run into CUDA/torch wheel issues on Colab, follow colab recommended torch install."
      ],
      "metadata": {
        "id": "g0g7H85LZLks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Utilities: load audio, play, save"
      ],
      "metadata": {
        "id": "_Ws6uwYJZYn1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZyzRn7vYtK8"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: download audio from URL\n",
        "def load_audio_from_url(url, target_sr=16000):\n",
        "    resp = requests.get(url)\n",
        "    audio_bytes = BytesIO(resp.content)\n",
        "    data, sr = sf.read(audio_bytes)\n",
        "    # convert stereo to mono if needed\n",
        "    if len(data.shape) > 1:\n",
        "        data = data.mean(axis=1)\n",
        "    # resample if needed\n",
        "    if sr != target_sr:\n",
        "        data = torchaudio.functional.resample(torch.tensor(data).unsqueeze(0), sr, target_sr).squeeze(0).numpy()\n",
        "        sr = target_sr\n",
        "    return data, sr\n",
        "\n",
        "# Helper: save numpy audio to file\n",
        "def save_audio_np(waveform, sr, outpath):\n",
        "    sf.write(outpath, waveform, sr)\n",
        "\n",
        "# Play audio in notebook\n",
        "def play_audio_file(path, autoplay=False):\n",
        "    display(Audio(path, autoplay=autoplay))"
      ],
      "metadata": {
        "id": "Iwl0wEP0ZazN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('utils loaded')\n"
      ],
      "metadata": {
        "id": "K7sqstCJZdXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://deepgram.com/ ->STT"
      ],
      "metadata": {
        "id": "ffW-w2SD3Rc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu -> model host-> latency | performance"
      ],
      "metadata": {
        "id": "GtaGG2P027L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) ASR: English — Whisper (Hugging Face `transformers` pipeline)"
      ],
      "metadata": {
        "id": "EcDDslJ4ZjvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We'll use the `pipeline('automatic-speech-recognition')` with an OpenAI Whisper model. Whisper is robust & multilingual.\n"
      ],
      "metadata": {
        "id": "Gw0azaB6Zlrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "print('Loading Whisper ASR pipeline (this will download model weights)')\n",
        "asr_whisper = pipeline('automatic-speech-recognition', model='openai/whisper-small')\n",
        "\n",
        "\n",
        "print('Transcribing...')\n",
        "res = asr_whisper('/content/LJ001-0002.wav')\n",
        "print('Transcription:', res['text'])\n",
        "# play_audio_file('en_example.wav') # Uncomment if 'en_example.wav' exists"
      ],
      "metadata": {
        "id": "g6Zzhc5PZkAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call recording -> audio split -> model -> Transcription\n",
        "\n",
        "# stt (model)-> opensource"
      ],
      "metadata": {
        "id": "qQEJonOJ4wKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##finetuning"
      ],
      "metadata": {
        "id": "f4SbfOFI6P6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8_TWCot6PbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_audio_file('LJ001-0002.wav')"
      ],
      "metadata": {
        "id": "XeHjQ8aicMwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Transcribing...')\n",
        "res = asr_whisper('/content/LJ001-0006.wav')\n",
        "print('Transcription:', res['text'])\n",
        "# play_audio_file('en_example.wav') # Uncomment if 'en_example.wav' exists"
      ],
      "metadata": {
        "id": "zt7dBueacP3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_audio_file('LJ001-0006.wav')"
      ],
      "metadata": {
        "id": "aGKiIUjBcYiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) ASR: Bengali — Wav2Vec2 models\n"
      ],
      "metadata": {
        "id": "0iQlAb1WZtS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many community models fine-tuned for Bengali exist (wav2vec2 / XLSR). We provide two options: a small demo model and a stronger XLSR fine-tuned checkpoint.\n",
        "\n",
        "- `arijitx/wav2vec2-xls-r-300m-bengali` (fine-tuned XLSR)\n",
        "- `ai4bharat/indicwav2vec_v1_bengali` (another community model)\n",
        "\n",
        "We'll use the pipeline API for simplicity.\n"
      ],
      "metadata": {
        "id": "gxppY6VbZv2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading Bengali ASR (wav2vec2) — may take a moment')\n",
        "asr_bengali = pipeline('automatic-speech-recognition', model='arijitx/wav2vec2-xls-r-300m-bengali')\n",
        "\n",
        "# Example Bengali short audio (upload your own or replace URL)\n",
        "# NOTE: Provide your own Bengali audio in Colab by uploading or using a URL\n",
        "# Here's a placeholder: you should replace with a proper Bengali audio file.\n",
        "\n",
        "# If you have a file 'bn_example.wav' in Colab, run:\n",
        "res_bn = asr_bengali('/content/train_barishal (1).wav')\n",
        "print('Bengali transcription:', res_bn['text'])"
      ],
      "metadata": {
        "id": "kkjXUqpRZsgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading Bengali ASR (wav2vec2) — may take a moment')\n",
        "asr_bengali = pipeline('automatic-speech-recognition', model='ai4bharat/indicwav2vec_v1_bengali')\n",
        "\n",
        "# Example Bengali short audio (upload your own or replace URL)\n",
        "# NOTE: Provide your own Bengali audio in Colab by uploading or using a URL\n",
        "# Here's a placeholder: you should replace with a proper Bengali audio file.\n",
        "\n",
        "# If you have a file 'bn_example.wav' in Colab, run:\n",
        "res_bn = asr_bengali('/content/train_barishal (1).wav')\n",
        "print('Bengali transcription:', res_bn['text'])"
      ],
      "metadata": {
        "id": "bgNvkpC-dBWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) TTS: English & Bengali\n"
      ],
      "metadata": {
        "id": "ifg57Fqzc2-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of transformers and accelerate\n",
        "# !pip install -q --upgrade transformers accelerate scipy soundfile\n",
        "import torch\n",
        "\n",
        "from transformers import VitsModel, AutoTokenizer, set_seed\n",
        "import scipy.io.wavfile as wavfile\n",
        "from IPython.display import Audio\n",
        "\n",
        "print(\"Loading MMS-TTS English model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\n",
        "model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\")\n",
        "\n",
        "text = \"Hello, students! This is a demo of English TTS using Facebook MMS.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Set seed for deterministic output\n",
        "set_seed(42)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).waveform\n",
        "\n",
        "waveform = outputs[0].cpu().numpy()\n",
        "sr = model.config.sampling_rate\n",
        "\n",
        "wavfile.write(\"mms_tts_eng.wav\", rate=sr, data=waveform)\n",
        "print(\"Saved as mms_tts_eng.wav — sample below:\")\n",
        "\n",
        "Audio(\"mms_tts_eng.wav\", rate=sr)\n"
      ],
      "metadata": {
        "id": "VtKBIokLeCke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# youtubevideo -> link -> audio -> transcription -> model(transcription)-> Refine .... -> summary"
      ],
      "metadata": {
        "id": "xJLkf2di-bJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assignment - voice to voice chatbot (stt , tts)\n",
        "Exam week (Virtual Try on)"
      ],
      "metadata": {
        "id": "h43pCDZS_eVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# speech to speech chatbot\n",
        "# speech -> stt -> text -> chatbot ->rag (agent) ->text -> tts -> speech"
      ],
      "metadata": {
        "id": "vrY4xzyo-Bdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Tips & Colab pointers for the class\n",
        "\n",
        "- **Use GPU runtime** in Colab for faster model downloads and generation (Runtime > Change runtime type > GPU).\n",
        "- **Model caching**: HF models will be cached in `~/.cache/huggingface` on first use.\n",
        "- **Rate limits**: For large class, students should run at different times or use small models to avoid HF rate limits.\n",
        "- **Audio formats**: Ensure input audio is 16 kHz mono for many ASR models (wav). Use `torchaudio` or `librosa` for resampling.\n",
        "- **Safety & Licenses**: Check model license (e.g., Coqui models may be non-commercial).\n",
        "\n",
        "---\n",
        "\n",
        "# Exercises for students\n",
        "# 1. Compare Whisper and wav2vec2 on a short Bengali-English mixed audio file.\n",
        "# 2. Fine-tune a small wav2vec2 model on a tiny Bengali dataset (use the `datasets` library).\n",
        "# 3. Try zero-shot voice cloning with coqui XTTS: record a 6-second clip and synthesize a paragraph.\n",
        "\n",
        "# End of notebook"
      ],
      "metadata": {
        "id": "0EEWFc9wefZE"
      }
    }
  ]
}