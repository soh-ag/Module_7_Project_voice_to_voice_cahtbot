{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "edadae84",
      "metadata": {
        "id": "edadae84"
      },
      "source": [
        "# Image Recognition & Generation with AI (Hugging Face â€” Offline)\n",
        "\n",
        "It uses **Hugging Face** models\n",
        "- Setup & installation\n",
        "- Image classification (ViT)\n",
        "- Object detection (DETR)\n",
        "- Image generation (Stable Diffusion via `diffusers`)\n",
        "- Image-to-image (img2img)\n",
        "\n",
        "Notes:\n",
        "- Downloading the models requires internet the first time; after that they are cached and can be used offline.\n",
        "- GPU is strongly recommended for Stable Diffusion. CPU will work for smaller demos (but slowly).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938ab0f1",
      "metadata": {
        "id": "938ab0f1"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# Run this cell once in your environment. In Colab or local notebooks, prefix with `!` to run shell commands. If you already have some libraries installed you can skip them.\n",
        "\n",
        "# Uncomment and run if needed:\n",
        "# !pip install --upgrade pip\n",
        "# !pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu117  # or CPU-only wheel if no GPU\n",
        "# !pip install transformers[torch] timm pillow\n",
        "!pip install diffusers transformers accelerate safetensors scipy ftfy supervision inference\n",
        "# !pip install -U \"git+https://github.com/huggingface/transformers\"\n",
        "\n",
        "# If using a local machine without CUDA, install CPU-only PyTorch as per https://pytorch.org\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461ae6ad",
      "metadata": {
        "id": "461ae6ad"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from IPython.display import display\n",
        "\n",
        "# small helper to show images inline\n",
        "def show_pil(img, title=None):\n",
        "    if title:\n",
        "        print(title)\n",
        "    display(img)\n",
        "\n",
        "# helper to download example images\n",
        "def load_image_from_url(url):\n",
        "    resp = requests.get(url)\n",
        "    return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
        "\n",
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset -> VIT -> Finetune"
      ],
      "metadata": {
        "id": "wjsXnfk5aaX7"
      },
      "id": "wjsXnfk5aaX7"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAkhTsAUaaD1"
      },
      "id": "QAkhTsAUaaD1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "95fec82e",
      "metadata": {
        "id": "95fec82e"
      },
      "source": [
        "## 1) Image Classification using ViT (Hugging Face `transformers`)\n",
        "\n",
        "We'll use a pre-trained Vision Transformer model and the associated feature extractor.\n",
        "\n",
        "Model: `google/vit-base-patch16-224` (image classification head trained on ImageNet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9a3d22",
      "metadata": {
        "id": "7b9a3d22"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "# load feature extractor and model\n",
        "feat_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model_vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224').to(device)\n",
        "model_vit.eval()\n",
        "\n",
        "# Example: classify an image\n",
        "img_url = 'https://t3.ftcdn.net/jpg/02/36/99/22/360_F_236992283_sNOxCVQeFLd5pdqaKGh8DRGMZy7P4XKm.jpg'  # replace with local path if desired\n",
        "img = load_image_from_url(img_url)\n",
        "show_pil(img, 'Input image')\n",
        "\n",
        "# preprocess\n",
        "inputs = feat_extractor(images=img, return_tensors='pt').to(device)\n",
        "\n",
        "# predict\n",
        "with torch.no_grad():\n",
        "    outputs = model_vit(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = logits.softmax(dim=-1)\n",
        "    top5 = torch.topk(probs, k=5)\n",
        "\n",
        "# decode labels\n",
        "id2label = model_vit.config.id2label\n",
        "for score, idx in zip(top5.values[0], top5.indices[0]):\n",
        "    print(f\"{id2label[int(idx.item())]}: {float(score):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: classify an image\n",
        "img_url = 'https://media.istockphoto.com/id/1098182434/photo/young-cat-scottish-straight.jpg?s=612x612&w=0&k=20&c=WP-SVdLfKH7nDV5FvXN8flUbo9CI0xtE775wm-eegE0='  # replace with local path if desired\n",
        "img = load_image_from_url(img_url)\n",
        "show_pil(img, 'Input image')\n",
        "\n",
        "# preprocess\n",
        "inputs = feat_extractor(images=img, return_tensors='pt').to(device)\n",
        "\n",
        "# predict\n",
        "with torch.no_grad():\n",
        "    outputs = model_vit(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = logits.softmax(dim=-1)\n",
        "    top5 = torch.topk(probs, k=5)\n",
        "\n",
        "# decode labels\n",
        "id2label = model_vit.config.id2label\n",
        "for score, idx in zip(top5.values[0], top5.indices[0]):\n",
        "    print(f\"{id2label[int(idx.item())]}: {float(score):.4f}\")"
      ],
      "metadata": {
        "id": "7OgxqZXK-7Na"
      },
      "id": "7OgxqZXK-7Na",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b0559b68",
      "metadata": {
        "id": "b0559b68"
      },
      "source": [
        "## 2) Object Detection using DETR (Hugging Face `transformers`)\n",
        "\n",
        "Model: `facebook/detr-resnet-50`\n",
        "\n",
        "This demonstrates bounding box detection and class labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4d696a",
      "metadata": {
        "id": "eb4d696a"
      },
      "outputs": [],
      "source": [
        "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "feat_det = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')\n",
        "model_detr = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50').to(device)\n",
        "model_detr.eval()\n",
        "\n",
        "# Load example image\n",
        "img_url = 'https://images.unsplash.com/photo-1518791841217-8f162f1e1131'\n",
        "img = load_image_from_url(img_url)\n",
        "show_pil(img, 'Object detection input')\n",
        "\n",
        "# prepare\n",
        "inputs = feat_det(images=img, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model_detr(**inputs)\n",
        "\n",
        "# post-process\n",
        "target_sizes = torch.tensor([img.size[::-1]])  # (height, width)\n",
        "results = feat_det.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n",
        "\n",
        "\n",
        "# plot\n",
        "fig, ax = plt.subplots(1, figsize=(12,8))\n",
        "ax.imshow(img)\n",
        "for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    x0, y0, x1, y1 = box\n",
        "    w, h = x1-x0, y1-y0\n",
        "    rect = patches.Rectangle((x0, y0), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    class_name = model_detr.config.id2label[int(label.item())]\n",
        "    ax.text(x0, y0, f\"{class_name}: {score:.2f}\", bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load example image\n",
        "img_url = 'https://petstrainingandboarding.com.au/wp-content/uploads/2017/05/15994040_ml.jpg'\n",
        "img = load_image_from_url(img_url)\n",
        "show_pil(img, 'Object detection input')\n",
        "\n",
        "# prepare\n",
        "inputs = feat_det(images=img, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model_detr(**inputs)\n",
        "\n",
        "# post-process\n",
        "target_sizes = torch.tensor([img.size[::-1]])  # (height, width)\n",
        "results = feat_det.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n",
        "\n",
        "\n",
        "# plot\n",
        "fig, ax = plt.subplots(1, figsize=(12,8))\n",
        "ax.imshow(img)\n",
        "for score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    x0, y0, x1, y1 = box\n",
        "    w, h = x1-x0, y1-y0\n",
        "    rect = patches.Rectangle((x0, y0), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    class_name = model_detr.config.id2label[int(label.item())]\n",
        "    ax.text(x0, y0, f\"{class_name}: {score:.2f}\", bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x4TN6CXxAbfo"
      },
      "id": "x4TN6CXxAbfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "from inference import get_model\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "url = \"https://media.roboflow.com/dog.jpeg\"\n",
        "image = Image.open(BytesIO(requests.get(url).content))\n",
        "\n",
        "model = get_model(\"rfdetr-base\")\n",
        "\n",
        "predictions = model.infer(image, confidence=0.5)[0]\n",
        "\n",
        "detections = sv.Detections.from_inference(predictions)\n",
        "\n",
        "labels = [prediction.class_name for prediction in predictions.predictions]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)\n",
        "annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)"
      ],
      "metadata": {
        "id": "miLd03SHA0KZ"
      },
      "id": "miLd03SHA0KZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_image(annotated_image)"
      ],
      "metadata": {
        "id": "XuOiYdDYBxXQ"
      },
      "id": "XuOiYdDYBxXQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Capability Sota | Latency | Model | cost"
      ],
      "metadata": {
        "id": "JPqxLlI7ev3A"
      },
      "id": "JPqxLlI7ev3A"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://petstrainingandboarding.com.au/wp-content/uploads/2017/05/15994040_ml.jpg\"\n",
        "image = Image.open(BytesIO(requests.get(url).content))\n",
        "\n",
        "model = get_model(\"rfdetr-base\")\n",
        "\n",
        "predictions = model.infer(image, confidence=0.5)[0]\n",
        "\n",
        "detections = sv.Detections.from_inference(predictions)\n",
        "\n",
        "labels = [prediction.class_name for prediction in predictions.predictions]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)\n",
        "annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)"
      ],
      "metadata": {
        "id": "6waoOLm5B5qt"
      },
      "id": "6waoOLm5B5qt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_image(annotated_image)"
      ],
      "metadata": {
        "id": "XMueEE2FCBPw"
      },
      "id": "XMueEE2FCBPw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a88adfe9",
      "metadata": {
        "id": "a88adfe9"
      },
      "source": [
        "\n",
        " ## 3) Image Generation with Stable Diffusion (`diffusers`)\n",
        "\n",
        "We'll use the `diffusers` pipeline. Model example: `runwayml/stable-diffusion-v1-5` or `stabilityai/stable-diffusion-2-1`.\n",
        "\n",
        "**Warning:** These models are large (~4-8GB). If you're on a CPU-only machine, generation will be slow. Consider smaller models or check `diffusers` for tiny checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e606e9e5",
      "metadata": {
        "id": "e606e9e5"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "sd_model_id = 'runwayml/stable-diffusion-v1-5'\n",
        "\n",
        "# If you have a GPU, use torch_dtype=torch.float16 to save memory\n",
        "pipe = StableDiffusionPipeline.from_pretrained(sd_model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# safety: disable NSFW checker to avoid blocking in teaching demo (only for trusted local environment)\n",
        "try:\n",
        "    pipe.safety_checker = None\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Simple prompt-based generation\n",
        "prompt = \"A watercolor painting of a small island with a single tree, high detail\"\n",
        "with torch.autocast(device.type if device.type!='cpu' else 'cpu'):\n",
        "    image = pipe(prompt, guidance_scale=7.5, num_inference_steps=25).images[0]\n",
        "\n",
        "show_pil(image, 'Generated image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b1265b",
      "metadata": {
        "id": "99b1265b"
      },
      "source": [
        "## 4) Image-to-image (img2img)\n",
        "\n",
        "Take an existing image and generate a variation. We'll use the same Stable Diffusion pipeline with an initial image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0a6898",
      "metadata": {
        "id": "dd0a6898"
      },
      "outputs": [],
      "source": [
        "init_img_url = 'https://images.unsplash.com/photo-1595598239736-223ad4fe7da5?fm=jpg&q=60&w=3000&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8Z3JlZW4lMjBhcmVhfGVufDB8fDB8fHww'\n",
        "init_img = load_image_from_url(init_img_url).resize((512,512))\n",
        "show_pil(init_img, 'init image')\n",
        "\n",
        "prompt = \"A dreamy oil painting version of this scene, soft brush strokes\"\n",
        "with torch.autocast(device.type if device.type!='cpu' else 'cpu'):\n",
        "    out = pipe(prompt=prompt, image=init_img, strength=0.7, guidance_scale=7.5, num_inference_steps=25)\n",
        "    img2 = out.images[0]\n",
        "\n",
        "show_pil(img2, 'img2img result')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1828f5f",
      "metadata": {
        "id": "f1828f5f"
      },
      "source": [
        "## 5) Zero-shot image classification using CLIP\n",
        "\n",
        "We can use CLIP to compute similarity between images and text labels (useful for custom classes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056ad7e7",
      "metadata": {
        "id": "056ad7e7"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "\n",
        "# sample image\n",
        "img_url = 'https://images.unsplash.com/photo-1518791841217-8f162f1e1131'\n",
        "img = load_image_from_url(img_url)\n",
        "\n",
        "# labels\n",
        "candidate_labels = [\"person\", \"car\", \"dog\", \"cat\", \"bicycle\", \"tree\"]\n",
        "inputs = clip_processor(text=candidate_labels, images=img, return_tensors='pt', padding=True).to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "for label, p in zip(candidate_labels, probs[0]):\n",
        "    print(f\"{label}: {float(p):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2edbc49",
      "metadata": {
        "id": "f2edbc49"
      },
      "source": [
        "## Tips for teaching & running locally\n",
        "\n",
        "- **Model caching**: First run requires downloading models. Ask students to run `huggingface-cli login` if they hit rate limits for some models.\n",
        "- **Smaller alternatives**: For classrooms without GPU, use smaller models (e.g., use `torchvision` pretrained models like mobilenet_v2 for fast classification).\n",
        "- **Memory**: For Stable Diffusion try `scheduler=EulerAncestralDiscreteScheduler` and fewer `num_inference_steps` (10â€“25) for speed.\n",
        "- **Safety & licenses**: Remind students to check model licenses and avoid generating unsafe content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba45413a",
      "metadata": {
        "id": "ba45413a"
      },
      "source": [
        "## Next steps / Exercises for students\n",
        "\n",
        "1. Fine-tune ViT on a tiny custom dataset (use `datasets` and `Trainer`).\n",
        "2. Replace Stable Diffusion with a lightweight diffusion model for faster generation.\n",
        "3. Build a small web app using Gradio to wrap the classifier + generator.\n",
        "\n",
        "---\n",
        "\n",
        "# End of notebook\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}